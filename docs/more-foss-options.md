# More FOSS Options for Data Aggregation and Automation

This document provides a list of additional free and open-source software (FOSS) options that you can self-host to further enhance your data aggregation and automation capabilities.

## Data Aggregation

*   **[Huginn](https://github.com/huginn/huginn):** Huginn is a system for building agents that perform automated tasks for you online. It can read the web, watch for events, and take actions on your behalf. Huginn's agents can create and consume events, propagating them along a directed graph. Think of it as a hackable version of IFTTT or Zapier on your own server.
*   **[Node-RED](https://nodered.org/):** Node-RED is a programming tool for wiring together hardware devices, APIs and online services in new and interesting ways. It provides a browser-based editor that makes it easy to wire together flows using the wide range of nodes in the palette that can be deployed to its runtime in a single-click.
*   **[RSS-Bridge](https://github.com/RSS-Bridge/rss-bridge):** RSS-Bridge is a PHP project capable of generating RSS and Atom feeds for websites that don't have one. It can be used on private servers or public instances.

## Automation

*   **[ActiveWorkflow](https://github.com/automatic-agent/active_workflow):** ActiveWorkflow is an intelligent process and workflow automation platform that can be used to automate business processes.
*   **[Beehive](https://github.com/muesli/beehive):** Beehive is a flexible event and agent system, which allows you to create your own agents that carry out automated tasks for you. It's a more modern take on Huginn, written in Go.
*   **[Kestra](https://github.com/kestra-io/kestra):** Kestra is an open-source, event-driven, and language-agnostic orchestrator. It allows you to build, schedule, and run complex workflows.

## Web Scraping

*   **[Portia](https://github.com/scrapinghub/portia):** Portia is a visual scraping tool that allows you to scrape websites without writing any code. You can use it to create spiders that will crawl websites and extract data.
*   **[Scrapy](https://scrapy.org/):** Scrapy is a fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing.
